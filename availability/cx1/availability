#!/usr/bin/env python2

import json
import getpass
import re
import sys
import subprocess
from subprocess import Popen, PIPE
from sets import Set


data = json.loads(subprocess.check_output(["/opt/pbs/default/bin/pbsnodes", "-av", "-F", "json" ] )) #.decode("ascii"))

count={ "16":0, "24":0, "28":0 }
queue= { "short":0, "general":0, "large":0, "capability":0 }

classes= {
	"v1_throughput24" : 0,
	"v1_throughput72" : 0,
	"v1_general24"    : 0,
	"v1_general72"    : 0,
	"v1_multinode24"  : 0,
	"v1_multinode48"  : 0,
	"v1_largemem24"   : 0,
	"v1_largemem48"   : 0,
	"v1_gpu24"        : 0,
	"v1_gpu48"        : 0,
	"v1_singlenode24" : 0,
	"v1_debug"        : 0,
	"exp_48_128_72"   : 0,
}

gpu_types_by_queue = dict()

def is_dedicated(ql):
	q=[]
	has_pq = False 
	for qq in ql:
		if ("pq" in qq) or ("med-bio" in qq):
			has_pq = True
		if qq not in [ "v1_throughput2a", "v1_general24a" ]:
			q.append(qq)
	if len(q)==1 and has_pq:
		return True
	else:
		return False
	
qlistlen = dict()

prioritising=False
for n in data["nodes"]:
	node=data["nodes"][n]
	state = node["state"]
	qlist =  node["resources_available"]["Qlist"].split(",")

	if ("down" in state)  or  ("offline" in state) or ("unknown" in state):
		# Ignore offline nodes
		pass
	else:
		for q  in qlist:

			if q not in classes:
				classes[q] = 0

			if q not in qlistlen:
				qlistlen[q] = qlist
			else:
				for qq in qlist:
					if qq not in qlistlen[q]: qlistlen[q].append(qq)

		if "jobs" not in node or len(node["jobs"])==0:
			for q in qlist:
				classes[q] += 1

		for q in qlist:
			if q not in gpu_types_by_queue: gpu_types_by_queue[q]=dict()
			if "ngpus" in node["resources_available"] and "gpu_type" in node["resources_available"]:
				gpu_type   = node["resources_available"]["gpu_type"]
				ngpus_tot  = int(node["resources_available"]["ngpus"])
				ngpus_used = 0
				if "ngpus" in node["resources_assigned"]:
					ngpus_used = int( node["resources_assigned"]["ngpus"])
				if gpu_type not in gpu_types_by_queue[q]:
					gpu_types_by_queue[q][gpu_type] = { "total" : ngpus_tot, "used" : ngpus_used }
				else:
					gpu_types_by_queue[q][gpu_type]["total"] += ngpus_tot
					gpu_types_by_queue[q][gpu_type]["used"]  += ngpus_used

who = getpass.getuser()
pqmemb=[]

for pq in classes.keys():
	if re.match("pq", pq):
		g = re.sub("^pq", "pq-", pq )
	elif pq == "med-bio":
		g = "pq-med-bio"
	else:
		g= None
	users = []
	if g:
		output = Popen(["/usr/bin/getent", "group", g], stdout=PIPE).communicate()[0].strip()
		output = output.split(':')[-1].split(',')
		users = output #users.union(output)
	if who in users:
		pqmemb.append(pq)


print ("")
print ( " Nodes available for throughput  : %3d (24hr) %3d (72hr)" % (classes["v1_throughput24"], classes["v1_throughput72"] ) )
print ( " Nodes available for general     : %3d (24hr) %3d (72hr)" % (classes["v1_general24"], classes["v1_general72"] ) )
print ( " Nodes available for singlenode  : %3d (24hr)  " % (classes["v1_singlenode24"], ) )
print ( " Nodes available for multinode   : %3d (24hr) %3d (48hr)" % (classes["v1_multinode24"], classes["v1_multinode48"], ) )
print ( " Nodes available for large memory: %3d (24hr) %3d (48hr) "  % (classes["v1_largemem24"], classes["v1_largemem48"], ) )
print ( " Nodes available for GPU         : %3d (24hr) %3d (48hr)"  % (classes["v1_gpu24"], classes["v1_gpu48"], ) )
print ( " Nodes available for debug       : %3d (30min) " % (classes["v1_debug"], ) )
print ( "")
print ( " Nodes available for 48c/128gb express  : %3d (72hr) " % (classes["exp_48_128_72" ] ) )
print ( " Nodes available for 32c/64gb express   : %3d (72hr) " % (classes["exp_32_64_72" ] ) )
print ( "")

for pq in sorted(pqmemb):
	if pq in classes:
		if not is_dedicated( qlistlen[pq] ) : 
			mode = "(shared)"
		else:	
			mode = "(dedicated)"
		print ( " Nodes available for %-11s : %3d   %s" % (pq, classes[pq], mode, ) )

print (" GPUs available:" )
print ("")
gt = gpu_types_by_queue["v1_gpu24"]
for k in sorted(gt.keys()):
	if k not in gpu_types_by_queue["v1_gpu48"]:
		gpu_types_by_queue["v1_gpu48"][k] = { "total":0, "used":0 }

	print( "%10s : %3d/%3d (24hr)\t%3d/%3d (48hr)" % ( k,  gpu_types_by_queue["v1_gpu24"][k]["total"] - gpu_types_by_queue["v1_gpu24"][k]["used"],  gpu_types_by_queue["v1_gpu24"][k]["total"],  gpu_types_by_queue["v1_gpu48"][k]["total"] -  gpu_types_by_queue["v1_gpu48"][k]["used"],  gpu_types_by_queue["v1_gpu48"][k]["total"] ))

print ("")
print (" For guidance only. You may not be eligible to run additional jobs if")
print (" you have reached the per-user job concurrency limit. ")
print ("")
print (" See our current job sizing guidance at http://bit.ly/2AInEIj" )
print ("")


